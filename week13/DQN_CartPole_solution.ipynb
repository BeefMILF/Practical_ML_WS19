{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_CartPole_solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccc-frankfurt/Practical_ML_WS19/blob/master/week13/DQN_CartPole_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRyizSSW2LDA",
        "colab_type": "text"
      },
      "source": [
        "This notebook is based on coursera's Practical RL course by National Research University Higher School of Economics, https://www.coursera.org/learn/practical-rl/home/welcome\n",
        "\n",
        "We will make a DQN-Learning agent to solve OpenAI Gym's CartPole problem.\n",
        "\n",
        "Q-Learning update equations:\n",
        "\n",
        "\n",
        "*   tabular: $$ Q(s,a) := (1 - \\alpha) \\cdot Q(s,a) + \\alpha \\cdot (r(s,a) + \\gamma \\cdot V(s')) \\\\ = Q(s,a) + \\alpha \\cdot (r(s,a) + \\gamma \\cdot V(s') - Q(s,a))$$\n",
        "*    DQN: $$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdVjjTaIlms4",
        "colab_type": "code",
        "outputId": "8b6d8242-8ec5-4425-acc3-c497b4123bc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "import random, math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "manualSeed = 999\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f39e0ce7710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqWiwab7tuP8",
        "colab_type": "code",
        "outputId": "b4b9eff7-4be3-47b2-d86e-1adaf376fe52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#XVFB will be launched if you run on a server\n",
        "import os\n",
        "if os.environ.get(\"DISPLAY\") is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    #!bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: DISPLAY=:1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsTERNDJnN4g",
        "colab_type": "code",
        "outputId": "c304a5be-a948-4724-d71c-61aebd2c2d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "!apt-get -qq -y install xvfb freeglut3-dev cmake swig ffmpeg> /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "%matplotlib inline"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (0.2.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.2.9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRp87KGKu8Ia",
        "colab_type": "code",
        "outputId": "db7948d0-8d99-427c-f195-d8d6e4af0e0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"CartPole-v0\").env\n",
        "s = env.reset()\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "print(n_actions)\n",
        "# continuous (!) valued observations: Cart Position, Cart Velocity, Pole Angle, Pole Velocity at Tip\n",
        "print(env.observation_space)\n",
        "print('state:')\n",
        "print(s)\n",
        "print(state_dim)\n",
        "plt.imshow(env.render('rgb_array'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "Box(4,)\n",
            "state:\n",
            "[ 0.02939001  0.02816018 -0.03161111  0.0266372 ]\n",
            "4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f39c0ea8940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H51R6ZQXglwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, n_actions, epsilon=0, device='cuda'):\n",
        "        \"\"\"A simple DQN agent\"\"\"\n",
        "        \n",
        "                         \n",
        "        # Create a network for approximate q-learning:\n",
        "        # what is the input dimension?\n",
        "        # Linear(128), followed by RELU activation\n",
        "        # Linear(128), followed by RELU activation\n",
        "        # Last linear output layer: what should be the number of units?\n",
        "        self.network = nn.Sequential(nn.Linear(state_dim, 128),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Linear(128, 128),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Linear(128,n_actions)).to(device)\n",
        "            \n",
        "        self.epsilon = epsilon\n",
        "        self.device = device\n",
        "        \n",
        "    def get_qvalues(self, state_t):\n",
        "        return self.network(state_t)\n",
        "    \n",
        "    def sample_action(self, qvalues):\n",
        "        \"\"\"Pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
        "        epsilon = self.epsilon\n",
        "        batch_size, n_actions = qvalues.shape\n",
        "        # choose random actions with np.random.choice\n",
        "        # there have to be batch_size of them - one for every batch instance\n",
        "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
        "        # for every batch instance calculate the index of the maximum qvalue\n",
        "        # hint: use the axis parameter of arg max\n",
        "        best_actions = qvalues.argmax(axis=-1)\n",
        "        # with probability epsilon should explore is 1 and else 0\n",
        "        should_explore = np.random.choice([0, 1], batch_size, p = [1-epsilon, epsilon])\n",
        "        # take random_actions is should_explore is 1 and best_actions else\n",
        "        return np.where(should_explore, random_actions, best_actions)\n",
        "    \n",
        "    def get_action(self, state_t):\n",
        "        # since in our case we learn on 1 sample only, we put the state into list to make batch_size=1\n",
        "        state_pt = torch.Tensor([state_t]).float().to(self.device)\n",
        "        qvalues = self.get_qvalues(state_pt).cpu().detach().numpy()\n",
        "        \n",
        "        # sample_action returns a list, so we take the first element, because our batch_size=1\n",
        "        return self.sample_action(qvalues)[0]\n",
        "    \n",
        "    def update(self, s, a, r, next_s, done):\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        loss = self.loss_function([s],[a],[r],[next_s],[done])\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def loss_function(self, states, actions, rewards, next_states, is_done):\n",
        "        states_tmp = torch.Tensor(states).float().to(self.device)\n",
        "        actions_tmp = np.array(actions) # assure its an array and not a list\n",
        "        rewards_tmp = torch.Tensor(rewards).float().to(self.device)\n",
        "        next_states_tmp = torch.Tensor(next_states).float().to(self.device)\n",
        "        is_done_tmp = torch.Tensor(is_done).float().to(self.device)\n",
        "        \n",
        "        is_not_done = 1. - is_done_tmp\n",
        "        gamma = 0.99\n",
        "\n",
        "        \"\"\"Take q-values for actions agent just took\"\"\"\n",
        "\n",
        "        current_qvalues = self.get_qvalues(states_tmp)\n",
        "        \n",
        "        # Create a one-hot target encoding\n",
        "        # empty one-hot matrix: (number of batch instances, number of actions)\n",
        "        one_hot_actions = np.zeros((actions_tmp.shape[0], n_actions))\n",
        "\n",
        "        # set target idx to 1 - target is the index of the chosen action here\n",
        "        one_hot_actions[np.arange(actions_tmp.shape[0]), actions_tmp] = 1.\n",
        "        one_hot_actions = torch.Tensor(one_hot_actions).float().to(self.device)\n",
        "        \n",
        "        # for every batch instance, get the qvalue for the chosen action\n",
        "        # hint: use one_hot_actions and current_q_values, do not forget to set keepdim=True\n",
        "        current_action_qvalues = torch.sum(one_hot_actions*current_qvalues, dim=1, keepdim=True)\n",
        "\n",
        "        # compute q-values for NEXT states\n",
        "        # hint: use next_states_ph\n",
        "        next_qvalues_target = self.get_qvalues(next_states_tmp)\n",
        "\n",
        "        # compute state values by taking max over next_qvalues_target for all actions\n",
        "        # hint: use next_qvalues_target and do not forget to set keepdim=True\n",
        "        next_state_values_target,_ = next_qvalues_target.max(dim=1, keepdim=True)\n",
        "\n",
        "        # compute Q_reference(s,a) as per r + gamma*v_next*is_not_done\n",
        "        # ATTENTION: think about what v_next is\n",
        "        reference_qvalues = rewards_tmp + gamma*next_state_values_target*is_not_done\n",
        "        # stop gradients of reference_qvalues\n",
        "        reference_qvalues = reference_qvalues.detach()\n",
        "\n",
        "        # Define loss function for sgd.\n",
        "        td_loss = (reference_qvalues - current_action_qvalues) ** 2\n",
        "        td_loss = torch.mean(td_loss)\n",
        "        \n",
        "        return td_loss\n",
        "        \n",
        "    def set_optimizer(self):\n",
        "        \n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0sBB3ZbsSqQ",
        "colab_type": "code",
        "outputId": "7bf46704-e704-49ac-de4f-68f6bec1c464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "agent = DQNAgent(state_dim, n_actions, epsilon=0.5, device='cuda')\n",
        "print(agent.network)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=4, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRsZdbgFhK5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# since it is an NN agent, we need to set an optimizer\n",
        "agent.set_optimizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7AysnffDtLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# update at each train step\n",
        "def play_and_train(env,agent,t_max=1000, train=True):\n",
        "    \"\"\"\n",
        "    This function should \n",
        "    - run a full game, actions given by agent's e-greedy policy\n",
        "    - train agent using agent.update(...) whenever it is possible\n",
        "    - return total reward\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        # get agent to pick action given state s.\n",
        "        a = agent.get_action(s)\n",
        "\n",
        "        next_s, r, done, _ = env.step(a)\n",
        "        \n",
        "        if train:\n",
        "            # train (update) agent for state s\n",
        "            agent.update(s, a, r, next_s, done)\n",
        "        \n",
        "        s = next_s\n",
        "        total_reward +=r\n",
        "        if done: break\n",
        "        \n",
        "    return total_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08joimEEytgF",
        "colab_type": "text"
      },
      "source": [
        "* __ mean reward__ is the average reward per game. For a correct implementation it may stay low for some 10 epochs, then start growing while oscilating insanely and converges by ~50-100 steps depending on the network architecture. \n",
        "* If it never reaches target score by the end of for loop, try increasing the number of hidden neurons or look at the epsilon.\n",
        "* __ epsilon__ - agent's willingness to explore. If you see that agent's already at < 0.01 epsilon before it's is at least 200, just reset it back to 0.1 - 0.5.\n",
        "* you may also try to tweak the learning rate of your optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJeVP7SoVguN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(100):\n",
        "    session_rewards = [play_and_train(env, agent, train=True) for _ in range(100)]\n",
        "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), agent.epsilon))\n",
        "    \n",
        "    agent.epsilon *= 0.99\n",
        "    assert agent.epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
        "    \n",
        "    if np.mean(session_rewards) > 300:\n",
        "        print (\"You Win!\")\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeFzEp4tuDMj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# record sessions\n",
        "import gym.wrappers\n",
        "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
        "# set epsilon to 0, since for testing no exploration is needed\n",
        "agent.epsilon = 0\n",
        "sessions = [play_and_train(env, agent, train=False) for _ in range(1)]\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgw17Mw2Wcya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show video\n",
        "# the visualization function from \n",
        "# https://star-ai.github.io/Rendering-OpenAi-Gym-in-Colaboratory/\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "import io\n",
        "import base64\n",
        "\n",
        "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
        "\n",
        "video = io.open(\"./videos/\"+video_names[-1], 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}