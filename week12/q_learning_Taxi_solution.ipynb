{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q-learning-Taxi_solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccc-frankfurt/Practical_ML_WS19/blob/master/week12/q_learning_Taxi_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRyizSSW2LDA",
        "colab_type": "text"
      },
      "source": [
        "This notebook is based on coursera's Practical RL course by National Research University Higher School of Economics, https://www.coursera.org/learn/practical-rl/home/welcome\n",
        "\n",
        "We will make a Q-Learning agent to solve OpenAI Gym's Taxi problem.\n",
        "\n",
        "Q-Learning update equation:\n",
        "\n",
        "\n",
        "*   tabular: $$ Q(s,a) := (1 - \\alpha) \\cdot Q(s,a) + \\alpha \\cdot (r(s,a) + \\gamma \\cdot V(s')) \\\\ = Q(s,a) + \\alpha \\cdot (r(s,a) + \\gamma \\cdot V(s') - Q(s,a))$$\n",
        "\n",
        "For more definitions, see also https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdVjjTaIlms4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import random, math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZQyTVzLTJva",
        "colab_type": "text"
      },
      "source": [
        "We pick a simple test environment for q-learning: picking up and dropping off customers.\n",
        "\n",
        "Note that for the Taxi environment your reward can be negative, see link for details: https://gym.openai.com/envs/Taxi-v3/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRp87KGKu8Ia",
        "colab_type": "code",
        "outputId": "a4bbf1c4-e0bf-434b-d542-fd696d328def",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "env.reset()\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print('Number of discrete actions:', n_actions, ' - pick up, drop off, up, down, left, right')\n",
        "# observations\n",
        "print(env.observation_space)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of discrete actions: 6\n",
            "Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AruLHu_TYi4",
        "colab_type": "text"
      },
      "source": [
        "Let's play a game (pick-up and drop off a customer) and see what happens if actions are just randomly sampled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5JI_2WR1cSE",
        "colab_type": "code",
        "outputId": "7aca906e-cb7e-46bf-aefa-d2c5c1eec52e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# render the observations to see how your taxi navigates through the maze\n",
        "s = env.reset()\n",
        "for _ in range(30):\n",
        "    # sample an action\n",
        "    a = env.action_space.sample()\n",
        "    # get the reward and the next state and whether the game has finished\n",
        "    next_s, r, done, _ = env.step(a)\n",
        "    # set the state to the next one\n",
        "    s = next_s\n",
        "    \n",
        "    print('reward: ',r)\n",
        "    env.render()\n",
        "    \n",
        "    if done: break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | :\u001b[43m \u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | :\u001b[43m \u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | :\u001b[43m \u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | :\u001b[43m \u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | :\u001b[43m \u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | :\u001b[43m \u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[43mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fprOgg9ZTnd4",
        "colab_type": "text"
      },
      "source": [
        "Let's now build an agent which will learn a better policy to pick up customers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1nsw7Ex0Kuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
        "        \"\"\"\n",
        "        Q-Learning Agent\n",
        "        based on http://ai.berkeley.edu/projects/release/reinforcement/v1/001/docs/qlearningAgents.html\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.get_legal_actions = get_legal_actions\n",
        "        # dictionary of expected rewards for (state,action) pairs\n",
        "        # it is a dictionary of dictionaries, because for every state,\n",
        "        # there could be multiple actions, each of which has qvalue\n",
        "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        # learning rate\n",
        "        self.alpha = alpha\n",
        "        # exploration-exploitation trade-off\n",
        "        self.epsilon = epsilon\n",
        "        # gamma - the future reward discount factor\n",
        "        self.discount = discount\n",
        "\n",
        "    def get_qvalue(self, state, action):\n",
        "        \"\"\" Returns Q(state,action) \"\"\"\n",
        "        return self._qvalues[state][action]\n",
        "\n",
        "    def set_qvalue(self,state,action,value):\n",
        "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
        "        self._qvalues[state][action] = value\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Compute your agent's estimate of V(s) using current q-values\n",
        "        V(s) = max_over_action Q(state,action) over possible actions.\n",
        "        Note: please take into account that q-values can be negative.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        #If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        # go through all possible actions, check the qvalue and return the max\n",
        "        \n",
        "        value = -np.inf\n",
        "        \n",
        "        for action in possible_actions:\n",
        "            tmp_val = self.get_qvalue(state, action)\n",
        "            if value < tmp_val:\n",
        "                value = tmp_val\n",
        "\n",
        "        return value\n",
        "\n",
        "    def get_best_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the best action to take in a state (using current q-values). \n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        #If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        # analog to the get_value method, with a different return\n",
        "        \n",
        "        value = -np.inf\n",
        "        best_action = None                                                                          \n",
        "        \n",
        "        for action in possible_actions:\n",
        "            tmp_val =self. get_qvalue(state, action)\n",
        "            if value < tmp_val:\n",
        "                value = tmp_val\n",
        "                best_action = action\n",
        "                                                                                  \n",
        "        return best_action\n",
        "        \n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the action to take in the current state, including exploration.  \n",
        "        With probability self.epsilon, we should take a random action.\n",
        "            otherwise - the best policy action.\n",
        "        \n",
        "        Note: To pick randomly from a list, use random.choice(list). \n",
        "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
        "              and compare it with your probability.\n",
        "        \"\"\"\n",
        "\n",
        "        # Pick Action\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        # agent parameters:\n",
        "        epsilon = self.epsilon\n",
        "        pick_best = np.random.rand() >= epsilon\n",
        "        \n",
        "        if pick_best:                                                                      \n",
        "            chosen_action = self.get_best_action(state)\n",
        "        else:\n",
        "            chosen_action = np.random.choice(possible_actions)                                                                      \n",
        "        \n",
        "        return chosen_action\n",
        "    \n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        You should do your Q-Value update here:\n",
        "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
        "           \n",
        "        Hint: use get_value method\n",
        "        \"\"\"\n",
        "\n",
        "        #agent parameters\n",
        "        gamma = self.discount\n",
        "        learning_rate = self.alpha\n",
        "        \n",
        "        value = (1 - learning_rate) * self.get_qvalue(state, action) \n",
        "        value += learning_rate * (reward + gamma * self.get_value(next_state))\n",
        "        \n",
        "        self.set_qvalue(state, action, value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0sBB3ZbsSqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize a new agent\n",
        "agent = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99,\n",
        "                       get_legal_actions = lambda s: range(n_actions))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7AysnffDtLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# update at each train step\n",
        "def play_and_train(env,agent,t_max=10**4):\n",
        "    \"\"\"\n",
        "    This function should \n",
        "    - run a full game, actions given by agent's e-greedy policy\n",
        "    - train agent using agent.update(...) whenever it is possible\n",
        "    - return total reward\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        # get agent to pick action given state s, get new state and reward, then update your agent\n",
        "        a = agent.get_action(s)\n",
        "        \n",
        "        next_s, r, done, _ = env.step(a)\n",
        "        \n",
        "        # train (update) agent for state s\n",
        "        agent.update(s, a, r, next_s, done)\n",
        "        \n",
        "        s = next_s\n",
        "        total_reward +=r\n",
        "        if done: break\n",
        "        \n",
        "    return total_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb5lHa_KZybJ",
        "colab_type": "text"
      },
      "source": [
        "Let us visualize the reward: play a game and update the agent, dicrease exploration (since the agent has already learned something) and plot the mean reward over the last ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJeVP7SoVguN",
        "colab_type": "code",
        "outputId": "b087807d-15ba-4ea3-b2e7-46938bc0e11a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "rewards = []\n",
        "for i in range(1000):\n",
        "    rewards.append(play_and_train(env, agent))\n",
        "    # we are making epsilon smaller \n",
        "    agent.epsilon *= 0.99\n",
        "    \n",
        "    if i %100 ==0:\n",
        "        clear_output(True)\n",
        "        print('eps =', agent.epsilon, 'mean reward =', np.mean(rewards[-10:]))\n",
        "        plt.plot(rewards)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eps = 2.9191091959171894e-05 mean reward = 7.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD4CAYAAAAEhuazAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wFwbLakXk6HiwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgw17Mw2Wcya",
        "colab_type": "code",
        "outputId": "9583641b-7dd5-435e-b98b-569f772b241e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# render the observations to see how your taxi navigates through the maze\n",
        "s = env.reset()\n",
        "for _ in range(30):\n",
        "    a = agent.get_best_action(s)\n",
        "        \n",
        "    next_s, r, done, _ = env.step(a)\n",
        "    \n",
        "    s = next_s\n",
        "    \n",
        "    env.render()\n",
        "    \n",
        "    if done: break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[42mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}