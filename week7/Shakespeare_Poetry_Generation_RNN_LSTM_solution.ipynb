{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shakespeare_Poetry_Generation_RNN_LSTM_solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccc-frankfurt/Practical_ML_WS19/blob/master/week7/Shakespeare_Poetry_Generation_RNN_LSTM_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BlaZc1G5m04",
        "colab_type": "text"
      },
      "source": [
        "# Shakespeare poetry generation with recurrent neural networks (RNNs)\n",
        "\n",
        "In this notebook we will take Shakespeares sonnets and train a recurrent neural network to generate poetry in a similar style. We will do so by training a character based simple RNN that takes in one character and learns to predict the likelihood of the next one. \n",
        "During inference we can use this trained network to give a random character to the model and let it sample an entire poem based on the previously sampled character (the network output) as input to the network's next time step.\n",
        "\n",
        "\n",
        "This notebook is loosely based on the following towards data science post: https://towardsdatascience.com/writing-like-shakespeare-with-machine-learning-in-pytorch-d77f851d910c . Here the author uses a PyTorch LSTM to do poetry generation and we have taken the liberty to use the author's data preprocessing steps in this notebook for convenience and have added a RNN from scratch implementation together with the PyTorch LSTM model for the later part in this notebook. \n",
        "\n",
        "After data loading, we will thus first learn how to implement a recurrent neural network in PyTorch, but with the equations written from scratch similar to our previous Numpy implementations. However, adding this custom math to the PyTorch neural network model will allow us to nevertheless leverage PyTorch's backward and automatic gradient methods so that we do not need to implement this ourselves anymore. This is very helpful if we ever plan on implementing custom functions/layers in the future. \n",
        "\n",
        "If you feel like you first want to implement both RNNs and LSTMs from scratch entirely using Numpy, including forward and gradient calculations, Andrew Ng's deeplearning.ai deep learning specialization on coursera has a sequence model course (course 5) with an excellent first notebook on this: https://www.coursera.org/learn/nlp-sequence-models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kinYD3010vdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "\n",
        "# Check whether GPU is available and can be used\n",
        "# if CUDA is found then device is set accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Consider changing your run-time to GPU or training will be slow.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stneSw5L77Ln",
        "colab_type": "text"
      },
      "source": [
        "## The data: Shakespeare's sonnets \n",
        "\n",
        "Shakespeare's sonnets can be found at the following URL featuring all of his works: http://shakespeare.mit.edu/\n",
        "\n",
        "For convenience reasons we have extracted all the plain text of the sonnets: https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt into a separate textfile and have added it to the class' repository. We will thus download it from there:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTxp46sNyKNQ",
        "colab_type": "code",
        "outputId": "108f392e-0e1d-4b06-de03-f266e1ab8c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/ccc-frankfurt/Practical_ML_WS19/master/week7/sonnets.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-25 15:50:13--  https://raw.githubusercontent.com/ccc-frankfurt/Practical_ML_WS19/master/week7/sonnets.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94081 (92K) [text/plain]\n",
            "Saving to: ‘sonnets.txt’\n",
            "\n",
            "\rsonnets.txt           0%[                    ]       0  --.-KB/s               \rsonnets.txt         100%[===================>]  91.88K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-11-25 15:50:13 (3.54 MB/s) - ‘sonnets.txt’ saved [94081/94081]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzimDdxq8oXk",
        "colab_type": "text"
      },
      "source": [
        "We can open the text file and print an excerpt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdSreQlxy11g",
        "colab_type": "code",
        "outputId": "21946b7a-7360-4c2e-d717-5799e9c82895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Open shakespeare text file and read the data\n",
        "with open('sonnets.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "    \n",
        "# print an excerpt of the text \n",
        "print(text[:200])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From fairest creatures we desire increase,\n",
            "That thereby beauty's rose might never die,\n",
            "But as the riper should by time decease,\n",
            "His tender heir might bear his memory:\n",
            "But thou contracted to thine own \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqofvzDN8u6Z",
        "colab_type": "text"
      },
      "source": [
        "As we are interested in a character based neural network, we will now create a mapping from the characters to numbers so that we can do our matrix calculations with numerical data. One such way is to simply replace every character with the corresponding integer in an alphabetical sequence. If we print our excerpt, we can now see the corresponding numerical values of each character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS8C0ndK0tce",
        "colab_type": "code",
        "outputId": "46ab67e4-ad82-442f-843d-8d58cf764913",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# We create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to integers\n",
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "# Encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])\n",
        "\n",
        "# Again showing the excerpt, but this time as integers \n",
        "encoded[:200]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3, 39, 13, 55, 35, 37, 56, 32, 39,  7, 51, 30, 35, 10, 39,  7, 56,\n",
              "       30, 40, 39,  7, 51, 35, 59,  7, 35,  0,  7, 51, 32, 39,  7, 35, 32,\n",
              "       15, 10, 39,  7, 56, 51,  7,  6,  4, 38, 29, 56, 30, 35, 30, 29,  7,\n",
              "       39,  7, 12, 53, 35, 12,  7, 56, 40, 30, 53, 31, 51, 35, 39, 13, 51,\n",
              "        7, 35, 55, 32, 18, 29, 30, 35, 15,  7,  8,  7, 39, 35,  0, 32,  7,\n",
              "        6,  4, 47, 40, 30, 35, 56, 51, 35, 30, 29,  7, 35, 39, 32, 52,  7,\n",
              "       39, 35, 51, 29, 13, 40, 48,  0, 35, 12, 53, 35, 30, 32, 55,  7, 35,\n",
              "        0,  7, 10,  7, 56, 51,  7,  6,  4, 14, 32, 51, 35, 30,  7, 15,  0,\n",
              "        7, 39, 35, 29,  7, 32, 39, 35, 55, 32, 18, 29, 30, 35, 12,  7, 56,\n",
              "       39, 35, 29, 32, 51, 35, 55,  7, 55, 13, 39, 53, 24,  4, 47, 40, 30,\n",
              "       35, 30, 29, 13, 40, 35, 10, 13, 15, 30, 39, 56, 10, 30,  7,  0, 35,\n",
              "       30, 13, 35, 30, 29, 32, 15,  7, 35, 13, 59, 15, 35])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZCdTzB39dNl",
        "colab_type": "text"
      },
      "source": [
        "### Data loader: batching\n",
        "\n",
        "We now have our entire text file encoded as integers, which serves as our dataset. Next, we will need to define our data loader, mainly the part that is missing, the random sampling of batches. Let us define this method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWq80TVX1DNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining method to make mini-batches for training\n",
        "def get_batches(arr, batch_size, seq_length):\n",
        "    # determine the flattened batch size, i.e. sequence length times batch size\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWDjvcQW92ze",
        "colab_type": "text"
      },
      "source": [
        "### Targets/Labels\n",
        "\n",
        "We will be treating our problem as a classification task, where given an input the task is to predict the likelihood of the next character, i.e. we choose the class/character with the highest probability of a SoftMax output. Our model's output is thus a vector containing a probability for each unique character.\n",
        "\n",
        "Since we want to be able to feed our model's output back as input for the next time step, we should also give the network a one-hot encoded character as the input instead of just an integer, similar to what we have seen on our lecture's last slide. \n",
        "This way the network gets as input a one-hot vector of length corresponding to the number of total unique characters and predicts the likelihood for each character as output (for the next character in the sequence). We will thus write a function that converts our encoded characters from integers to one-hot vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l9QohlqnU-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GHm8hBaALrD",
        "colab_type": "text"
      },
      "source": [
        "## A simple RNN\n",
        "\n",
        "We will start with writing a simple RNN in PyTorch. To get a better understanding of how the RNN model works, we will not be using PyTorch's convenience RNN implementation, but write the main portion by hand ourselves. We will later use the convenience functions for the much more complicated LSTMs. \n",
        "\n",
        "Note that we could in principle do the same thing in pure Numpy but the advantage of implementing the forward logic in PyTorch is that we can use the automatic differentation for our backward pass and we do not need to implement the backpropagation through time ourselves. \n",
        "\n",
        "What we will learn here is:\n",
        "1. How to write a recurrent neural network (the forward pass)\n",
        "2. How to implement custom mathematical equations in the forward pass of a PyTorch model and leverage the automatic backward. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgkv_TmovgBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, chars, device, hidden_sz, drop_prob=0.5):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        # we already have this code on the top, but giving it to our model \n",
        "        # will be convenient for doing predictions later\n",
        "        # i.e. doing conversions from text to integers to one-hot & vice-versa\n",
        "        self.n_chars = len(chars)\n",
        "        self.int2char = dict(enumerate(chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        self.hidden_sz = hidden_sz\n",
        "        \n",
        "        # Note that this class inherits from the torch neural network class\n",
        "        # Instead of using a pre-built function we will write the math ourselves\n",
        "        # For this reason we will first need to define \"Parameters()\", that \n",
        "        # the PyTorch graph keeps track of and can optimize. In other words,\n",
        "        # let's give our class the weights & the bias that the RNN will need. \n",
        "        self.weight_ih = Parameter(torch.Tensor(self.n_chars, self.hidden_sz))\n",
        "        self.weight_hh = Parameter(torch.Tensor(self.hidden_sz, self.hidden_sz))\n",
        "        self.bias_hh = Parameter(torch.Tensor(self.hidden_sz))\n",
        "        \n",
        "        # Now that we have defined the RNN cell, let us define the output layer\n",
        "        # We will use a dropout layer to prevent overfitting and then \n",
        "        # follow with a conventional linear layer (matrix multiplication) that \n",
        "        # maps the RNN cell's output (the hidden state of the network) to the \n",
        "        # class output. Remembert that the class output corresponds to a \n",
        "        # vector of length of unique characters. \n",
        "        \n",
        "        # define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        # define the final, fully-connected output layer. We can use a \n",
        "        # PyTorch nn function here (or you could add the corresponding math\n",
        "        # below and assign an additional weight & bias at the top). \n",
        "        # We can see that we can create very custom models this way\n",
        "        self.fc = nn.Linear(self.hidden_sz, self.n_chars)\n",
        "        \n",
        "        # We have assigned the Parameters above, but we will need to also \n",
        "        # initialize them. Let's write a function for that and initialize\n",
        "        # our weights and bias. \n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.weight_ih)\n",
        "        nn.init.xavier_uniform_(self.weight_hh)\n",
        "        nn.init.zeros_(self.bias_hh)\n",
        "    \n",
        "    def forward(self, x, h_t):\n",
        "        \"\"\"Assumes x is of shape (batch, sequence, feature)\"\"\"\n",
        "        bs, seq_sz, _ = x.size()\n",
        "        hidden_seq = []\n",
        "        \n",
        "        # Given an input and an initial hidden state, calculate the next hidden\n",
        "        # state for each sequence element.\n",
        "        # We append all the hidden states to a list (similar to a batch size)\n",
        "        # so that we can concatenate them in the batch and feed them to our\n",
        "        # last linear layer all in parallel to avoid looping through the final\n",
        "        # output layer as there is no more dependence on other time steps. \n",
        "        for t in range(seq_sz):\n",
        "            x_t = x[:, t, :]\n",
        "            h_t = torch.tanh(x_t @ self.weight_ih + h_t @ self.weight_hh + self.bias_hh)\n",
        "            hidden_seq.append(h_t.unsqueeze(0))\n",
        "            \n",
        "        # Do the concatenation and reshaping for convenience\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "        \n",
        "        # Stack up the RNN outputs using view so that we can process the last \n",
        "        # layer in parallel\n",
        "        r_output = hidden_seq.contiguous().view(-1, self.hidden_sz)\n",
        "        \n",
        "        # pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Calculate fully connected layer output that yields our class vector\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, h_t\n",
        "    \n",
        "    def init_hidden(self, batch_size=1):\n",
        "        ''' Initializes hidden state '''\n",
        "        # This is a convenience function so that we can initialize a hidden\n",
        "        # state to zero when we start prediction on a sequence. Every further\n",
        "        # step will then depend on the previous hidden state. \n",
        "        \n",
        "        # Create two new tensors with sizes batch_size x n_hidden,\n",
        "        # initialized to zero for hidden the RNN's hidden state.\n",
        "        weight = next(self.parameters()).data\n",
        "        h_t = weight.new(batch_size, self.hidden_sz).zero_().to(device)\n",
        "        \n",
        "        return h_t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhNyXgWuFK2T",
        "colab_type": "text"
      },
      "source": [
        "The only thing missing is our training loop. It will look very similar to everything we have previously written, with two main differences:\n",
        "\n",
        "1. Our model is now also dependent on the hidden state and thus takes it as input and returns it as an additional output. \n",
        "2. Because we are using a recurrent neural network we will need to give our \"loss.backward()\" a \"retain_graph=True\" flag in order for it to log the history and be able to compute the backpropagation through time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSrhA4lR2EaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Declaring the train method\n",
        "def train(model, data, device, optimizer, criterion, epochs=10, batch_size=10,\n",
        "          seq_length=50, clip=5):\n",
        "    model.train()\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # initialize first hidden states with zeros\n",
        "        h = model.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            # One-hot encode our data, make them torch tensors & cast to device\n",
        "            x = one_hot_encode(x, model.n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            model.zero_grad()\n",
        "            \n",
        "            # get the output and hidden state from the model\n",
        "            output, h = model(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            # because we have flattened our batch and sequence in the model to \n",
        "            # be able to speed up the connection of the last fully-connected \n",
        "            # layer we now also need to view/flatten our target here\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward(retain_graph=True)\n",
        "            \n",
        "            # we use an additional trick of clipping gradients to avoid \n",
        "            # exploding gradients, which is a prominent problem in RNNs, just\n",
        "            # as the opposite problem of vanishing gradients.\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "            \n",
        "            \n",
        "        print(\"Epoch: {}/{}:\".format(epoch + 1, epochs),\n",
        "              \"Loss: {:.4f}:\".format(loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFKl2jWi2GeC",
        "colab_type": "code",
        "outputId": "a3996f58-4fcb-461c-fb1c-a5bc3e82fd5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define the model\n",
        "n_hidden=512\n",
        "model = RNN(chars, device, n_hidden).to(device)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "epochs = 300 # start with 50 or similar if you are debugging \n",
        "# train much longer if you want good results\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# train the model\n",
        "train(model, encoded, device, optimizer, criterion, epochs=epochs,\n",
        "      batch_size=batch_size, seq_length=seq_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/300: Loss: 3.3042:\n",
            "Epoch: 2/300: Loss: 3.1995:\n",
            "Epoch: 3/300: Loss: 4.4711:\n",
            "Epoch: 4/300: Loss: 3.2538:\n",
            "Epoch: 5/300: Loss: 3.1579:\n",
            "Epoch: 6/300: Loss: 3.1302:\n",
            "Epoch: 7/300: Loss: 3.0931:\n",
            "Epoch: 8/300: Loss: 3.0188:\n",
            "Epoch: 9/300: Loss: 2.9718:\n",
            "Epoch: 10/300: Loss: 2.9128:\n",
            "Epoch: 11/300: Loss: 2.8601:\n",
            "Epoch: 12/300: Loss: 2.8356:\n",
            "Epoch: 13/300: Loss: 2.7667:\n",
            "Epoch: 14/300: Loss: 2.7172:\n",
            "Epoch: 15/300: Loss: 2.6721:\n",
            "Epoch: 16/300: Loss: 2.6711:\n",
            "Epoch: 17/300: Loss: 2.6304:\n",
            "Epoch: 18/300: Loss: 2.5803:\n",
            "Epoch: 19/300: Loss: 2.5593:\n",
            "Epoch: 20/300: Loss: 2.5092:\n",
            "Epoch: 21/300: Loss: 2.4768:\n",
            "Epoch: 22/300: Loss: 2.4742:\n",
            "Epoch: 23/300: Loss: 2.4302:\n",
            "Epoch: 24/300: Loss: 2.4165:\n",
            "Epoch: 25/300: Loss: 2.3917:\n",
            "Epoch: 26/300: Loss: 2.3595:\n",
            "Epoch: 27/300: Loss: 2.3875:\n",
            "Epoch: 28/300: Loss: 2.3373:\n",
            "Epoch: 29/300: Loss: 2.3240:\n",
            "Epoch: 30/300: Loss: 2.3092:\n",
            "Epoch: 31/300: Loss: 2.2933:\n",
            "Epoch: 32/300: Loss: 2.2791:\n",
            "Epoch: 33/300: Loss: 2.2782:\n",
            "Epoch: 34/300: Loss: 2.2564:\n",
            "Epoch: 35/300: Loss: 2.2594:\n",
            "Epoch: 36/300: Loss: 2.2381:\n",
            "Epoch: 37/300: Loss: 2.2279:\n",
            "Epoch: 38/300: Loss: 2.2201:\n",
            "Epoch: 39/300: Loss: 2.2050:\n",
            "Epoch: 40/300: Loss: 2.1995:\n",
            "Epoch: 41/300: Loss: 2.1940:\n",
            "Epoch: 42/300: Loss: 2.1850:\n",
            "Epoch: 43/300: Loss: 2.1810:\n",
            "Epoch: 44/300: Loss: 2.1730:\n",
            "Epoch: 45/300: Loss: 2.2390:\n",
            "Epoch: 46/300: Loss: 2.2077:\n",
            "Epoch: 47/300: Loss: 2.1786:\n",
            "Epoch: 48/300: Loss: 2.2046:\n",
            "Epoch: 49/300: Loss: 2.1823:\n",
            "Epoch: 50/300: Loss: 2.1580:\n",
            "Epoch: 51/300: Loss: 2.1464:\n",
            "Epoch: 52/300: Loss: 2.1346:\n",
            "Epoch: 53/300: Loss: 2.1272:\n",
            "Epoch: 54/300: Loss: 2.1222:\n",
            "Epoch: 55/300: Loss: 2.1127:\n",
            "Epoch: 56/300: Loss: 2.1132:\n",
            "Epoch: 57/300: Loss: 2.1069:\n",
            "Epoch: 58/300: Loss: 2.1035:\n",
            "Epoch: 59/300: Loss: 2.1016:\n",
            "Epoch: 60/300: Loss: 2.0945:\n",
            "Epoch: 61/300: Loss: 2.0898:\n",
            "Epoch: 62/300: Loss: 2.1370:\n",
            "Epoch: 63/300: Loss: 2.1052:\n",
            "Epoch: 64/300: Loss: 2.0921:\n",
            "Epoch: 65/300: Loss: 2.0848:\n",
            "Epoch: 66/300: Loss: 2.0740:\n",
            "Epoch: 67/300: Loss: 2.0612:\n",
            "Epoch: 68/300: Loss: 2.0642:\n",
            "Epoch: 69/300: Loss: 2.0578:\n",
            "Epoch: 70/300: Loss: 2.0495:\n",
            "Epoch: 71/300: Loss: 2.0410:\n",
            "Epoch: 72/300: Loss: 2.0404:\n",
            "Epoch: 73/300: Loss: 2.0355:\n",
            "Epoch: 74/300: Loss: 2.0324:\n",
            "Epoch: 75/300: Loss: 2.0264:\n",
            "Epoch: 76/300: Loss: 2.0339:\n",
            "Epoch: 77/300: Loss: 2.0218:\n",
            "Epoch: 78/300: Loss: 2.0292:\n",
            "Epoch: 79/300: Loss: 2.0047:\n",
            "Epoch: 80/300: Loss: 2.0728:\n",
            "Epoch: 81/300: Loss: 2.0277:\n",
            "Epoch: 82/300: Loss: 2.0252:\n",
            "Epoch: 83/300: Loss: 2.0026:\n",
            "Epoch: 84/300: Loss: 1.9969:\n",
            "Epoch: 85/300: Loss: 1.9888:\n",
            "Epoch: 86/300: Loss: 1.9825:\n",
            "Epoch: 87/300: Loss: 1.9776:\n",
            "Epoch: 88/300: Loss: 1.9816:\n",
            "Epoch: 89/300: Loss: 1.9701:\n",
            "Epoch: 90/300: Loss: 1.9688:\n",
            "Epoch: 91/300: Loss: 1.9570:\n",
            "Epoch: 92/300: Loss: 1.9571:\n",
            "Epoch: 93/300: Loss: 1.9405:\n",
            "Epoch: 94/300: Loss: 1.9529:\n",
            "Epoch: 95/300: Loss: 1.9371:\n",
            "Epoch: 96/300: Loss: 1.9400:\n",
            "Epoch: 97/300: Loss: 1.9318:\n",
            "Epoch: 98/300: Loss: 1.9223:\n",
            "Epoch: 99/300: Loss: 1.9225:\n",
            "Epoch: 100/300: Loss: 1.9232:\n",
            "Epoch: 101/300: Loss: 1.9037:\n",
            "Epoch: 102/300: Loss: 1.9187:\n",
            "Epoch: 103/300: Loss: 1.9276:\n",
            "Epoch: 104/300: Loss: 1.9111:\n",
            "Epoch: 105/300: Loss: 1.9016:\n",
            "Epoch: 106/300: Loss: 1.9002:\n",
            "Epoch: 107/300: Loss: 1.8801:\n",
            "Epoch: 108/300: Loss: 1.8837:\n",
            "Epoch: 109/300: Loss: 1.8836:\n",
            "Epoch: 110/300: Loss: 1.8843:\n",
            "Epoch: 111/300: Loss: 1.8785:\n",
            "Epoch: 112/300: Loss: 1.8652:\n",
            "Epoch: 113/300: Loss: 1.8698:\n",
            "Epoch: 114/300: Loss: 1.8620:\n",
            "Epoch: 115/300: Loss: 1.8515:\n",
            "Epoch: 116/300: Loss: 1.8560:\n",
            "Epoch: 117/300: Loss: 1.8518:\n",
            "Epoch: 118/300: Loss: 1.8348:\n",
            "Epoch: 119/300: Loss: 1.8290:\n",
            "Epoch: 120/300: Loss: 1.8341:\n",
            "Epoch: 121/300: Loss: 1.8199:\n",
            "Epoch: 122/300: Loss: 1.8338:\n",
            "Epoch: 123/300: Loss: 1.8257:\n",
            "Epoch: 124/300: Loss: 1.8284:\n",
            "Epoch: 125/300: Loss: 1.8013:\n",
            "Epoch: 126/300: Loss: 1.8186:\n",
            "Epoch: 127/300: Loss: 1.8162:\n",
            "Epoch: 128/300: Loss: 1.7997:\n",
            "Epoch: 129/300: Loss: 1.8114:\n",
            "Epoch: 130/300: Loss: 1.8134:\n",
            "Epoch: 131/300: Loss: 1.7961:\n",
            "Epoch: 132/300: Loss: 1.7945:\n",
            "Epoch: 133/300: Loss: 1.7794:\n",
            "Epoch: 134/300: Loss: 1.7723:\n",
            "Epoch: 135/300: Loss: 1.7455:\n",
            "Epoch: 136/300: Loss: 1.7445:\n",
            "Epoch: 137/300: Loss: 1.7632:\n",
            "Epoch: 138/300: Loss: 1.7563:\n",
            "Epoch: 139/300: Loss: 1.7378:\n",
            "Epoch: 140/300: Loss: 1.7311:\n",
            "Epoch: 141/300: Loss: 1.7328:\n",
            "Epoch: 142/300: Loss: 1.7393:\n",
            "Epoch: 143/300: Loss: 1.7404:\n",
            "Epoch: 144/300: Loss: 1.7516:\n",
            "Epoch: 145/300: Loss: 1.7314:\n",
            "Epoch: 146/300: Loss: 1.7255:\n",
            "Epoch: 147/300: Loss: 1.7249:\n",
            "Epoch: 148/300: Loss: 1.7159:\n",
            "Epoch: 149/300: Loss: 1.6996:\n",
            "Epoch: 150/300: Loss: 1.7104:\n",
            "Epoch: 151/300: Loss: 1.6895:\n",
            "Epoch: 152/300: Loss: 1.6816:\n",
            "Epoch: 153/300: Loss: 1.6886:\n",
            "Epoch: 154/300: Loss: 1.7203:\n",
            "Epoch: 155/300: Loss: 1.6896:\n",
            "Epoch: 156/300: Loss: 1.7072:\n",
            "Epoch: 157/300: Loss: 1.7140:\n",
            "Epoch: 158/300: Loss: 1.6872:\n",
            "Epoch: 159/300: Loss: 1.6707:\n",
            "Epoch: 160/300: Loss: 1.6575:\n",
            "Epoch: 161/300: Loss: 1.6710:\n",
            "Epoch: 162/300: Loss: 1.6466:\n",
            "Epoch: 163/300: Loss: 1.6461:\n",
            "Epoch: 164/300: Loss: 1.6272:\n",
            "Epoch: 165/300: Loss: 1.6545:\n",
            "Epoch: 166/300: Loss: 1.6717:\n",
            "Epoch: 167/300: Loss: 1.6484:\n",
            "Epoch: 168/300: Loss: 1.6458:\n",
            "Epoch: 169/300: Loss: 1.6575:\n",
            "Epoch: 170/300: Loss: 1.6305:\n",
            "Epoch: 171/300: Loss: 1.6176:\n",
            "Epoch: 172/300: Loss: 1.6157:\n",
            "Epoch: 173/300: Loss: 1.6265:\n",
            "Epoch: 174/300: Loss: 1.6060:\n",
            "Epoch: 175/300: Loss: 1.5887:\n",
            "Epoch: 176/300: Loss: 1.5766:\n",
            "Epoch: 177/300: Loss: 1.5909:\n",
            "Epoch: 178/300: Loss: 1.5923:\n",
            "Epoch: 179/300: Loss: 1.5874:\n",
            "Epoch: 180/300: Loss: 1.5513:\n",
            "Epoch: 181/300: Loss: 1.5577:\n",
            "Epoch: 182/300: Loss: 1.5698:\n",
            "Epoch: 183/300: Loss: 1.6017:\n",
            "Epoch: 184/300: Loss: 1.5964:\n",
            "Epoch: 185/300: Loss: 1.5842:\n",
            "Epoch: 186/300: Loss: 1.5634:\n",
            "Epoch: 187/300: Loss: 1.5615:\n",
            "Epoch: 188/300: Loss: 1.5522:\n",
            "Epoch: 189/300: Loss: 1.5419:\n",
            "Epoch: 190/300: Loss: 1.5484:\n",
            "Epoch: 191/300: Loss: 1.5451:\n",
            "Epoch: 192/300: Loss: 1.5493:\n",
            "Epoch: 193/300: Loss: 1.5373:\n",
            "Epoch: 194/300: Loss: 1.5413:\n",
            "Epoch: 195/300: Loss: 1.5266:\n",
            "Epoch: 196/300: Loss: 1.5339:\n",
            "Epoch: 197/300: Loss: 1.5380:\n",
            "Epoch: 198/300: Loss: 1.5111:\n",
            "Epoch: 199/300: Loss: 1.5191:\n",
            "Epoch: 200/300: Loss: 1.5039:\n",
            "Epoch: 201/300: Loss: 1.5048:\n",
            "Epoch: 202/300: Loss: 1.5032:\n",
            "Epoch: 203/300: Loss: 1.5023:\n",
            "Epoch: 204/300: Loss: 1.5182:\n",
            "Epoch: 205/300: Loss: 1.5180:\n",
            "Epoch: 206/300: Loss: 1.5387:\n",
            "Epoch: 207/300: Loss: 1.4982:\n",
            "Epoch: 208/300: Loss: 1.4898:\n",
            "Epoch: 209/300: Loss: 1.5038:\n",
            "Epoch: 210/300: Loss: 1.4725:\n",
            "Epoch: 211/300: Loss: 1.4634:\n",
            "Epoch: 212/300: Loss: 1.4712:\n",
            "Epoch: 213/300: Loss: 1.4655:\n",
            "Epoch: 214/300: Loss: 1.4578:\n",
            "Epoch: 215/300: Loss: 1.4750:\n",
            "Epoch: 216/300: Loss: 1.4638:\n",
            "Epoch: 217/300: Loss: 1.4669:\n",
            "Epoch: 218/300: Loss: 1.4823:\n",
            "Epoch: 219/300: Loss: 1.4781:\n",
            "Epoch: 220/300: Loss: 1.4541:\n",
            "Epoch: 221/300: Loss: 1.4485:\n",
            "Epoch: 222/300: Loss: 1.4666:\n",
            "Epoch: 223/300: Loss: 1.4551:\n",
            "Epoch: 224/300: Loss: 1.4346:\n",
            "Epoch: 225/300: Loss: 1.4328:\n",
            "Epoch: 226/300: Loss: 1.4328:\n",
            "Epoch: 227/300: Loss: 1.4444:\n",
            "Epoch: 228/300: Loss: 1.4697:\n",
            "Epoch: 229/300: Loss: 1.4828:\n",
            "Epoch: 230/300: Loss: 1.4760:\n",
            "Epoch: 231/300: Loss: 1.4574:\n",
            "Epoch: 232/300: Loss: 1.4148:\n",
            "Epoch: 233/300: Loss: 1.3990:\n",
            "Epoch: 234/300: Loss: 1.4347:\n",
            "Epoch: 235/300: Loss: 1.4332:\n",
            "Epoch: 236/300: Loss: 1.4416:\n",
            "Epoch: 237/300: Loss: 1.4345:\n",
            "Epoch: 238/300: Loss: 1.4272:\n",
            "Epoch: 239/300: Loss: 1.3974:\n",
            "Epoch: 240/300: Loss: 1.4006:\n",
            "Epoch: 241/300: Loss: 1.3754:\n",
            "Epoch: 242/300: Loss: 1.3766:\n",
            "Epoch: 243/300: Loss: 1.3815:\n",
            "Epoch: 244/300: Loss: 1.4048:\n",
            "Epoch: 245/300: Loss: 1.3922:\n",
            "Epoch: 246/300: Loss: 1.3956:\n",
            "Epoch: 247/300: Loss: 1.4037:\n",
            "Epoch: 248/300: Loss: 1.3943:\n",
            "Epoch: 249/300: Loss: 1.4216:\n",
            "Epoch: 250/300: Loss: 1.4019:\n",
            "Epoch: 251/300: Loss: 1.4317:\n",
            "Epoch: 252/300: Loss: 1.3976:\n",
            "Epoch: 253/300: Loss: 1.3999:\n",
            "Epoch: 254/300: Loss: 1.3989:\n",
            "Epoch: 255/300: Loss: 1.4144:\n",
            "Epoch: 256/300: Loss: 1.3821:\n",
            "Epoch: 257/300: Loss: 1.3863:\n",
            "Epoch: 258/300: Loss: 1.3865:\n",
            "Epoch: 259/300: Loss: 1.3683:\n",
            "Epoch: 260/300: Loss: 1.3692:\n",
            "Epoch: 261/300: Loss: 1.3640:\n",
            "Epoch: 262/300: Loss: 1.4090:\n",
            "Epoch: 263/300: Loss: 1.3883:\n",
            "Epoch: 264/300: Loss: 1.3643:\n",
            "Epoch: 265/300: Loss: 1.3318:\n",
            "Epoch: 266/300: Loss: 1.3093:\n",
            "Epoch: 267/300: Loss: 1.3412:\n",
            "Epoch: 268/300: Loss: 1.3123:\n",
            "Epoch: 269/300: Loss: 1.3088:\n",
            "Epoch: 270/300: Loss: 1.3136:\n",
            "Epoch: 271/300: Loss: 1.3339:\n",
            "Epoch: 272/300: Loss: 1.3573:\n",
            "Epoch: 273/300: Loss: 1.3593:\n",
            "Epoch: 274/300: Loss: 1.3448:\n",
            "Epoch: 275/300: Loss: 1.3178:\n",
            "Epoch: 276/300: Loss: 1.3014:\n",
            "Epoch: 277/300: Loss: 1.3201:\n",
            "Epoch: 278/300: Loss: 1.3354:\n",
            "Epoch: 279/300: Loss: 1.3027:\n",
            "Epoch: 280/300: Loss: 1.3356:\n",
            "Epoch: 281/300: Loss: 1.3001:\n",
            "Epoch: 282/300: Loss: 1.3009:\n",
            "Epoch: 283/300: Loss: 1.3070:\n",
            "Epoch: 284/300: Loss: 1.2965:\n",
            "Epoch: 285/300: Loss: 1.2937:\n",
            "Epoch: 286/300: Loss: 1.2797:\n",
            "Epoch: 287/300: Loss: 1.2720:\n",
            "Epoch: 288/300: Loss: 1.2520:\n",
            "Epoch: 289/300: Loss: 1.2646:\n",
            "Epoch: 290/300: Loss: 1.2542:\n",
            "Epoch: 291/300: Loss: 1.2423:\n",
            "Epoch: 292/300: Loss: 1.2355:\n",
            "Epoch: 293/300: Loss: 1.2346:\n",
            "Epoch: 294/300: Loss: 1.2914:\n",
            "Epoch: 295/300: Loss: 1.2540:\n",
            "Epoch: 296/300: Loss: 1.2734:\n",
            "Epoch: 297/300: Loss: 1.2765:\n",
            "Epoch: 298/300: Loss: 1.2567:\n",
            "Epoch: 299/300: Loss: 1.2740:\n",
            "Epoch: 300/300: Loss: 1.2819:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIndlrMSGMmZ",
        "colab_type": "text"
      },
      "source": [
        "You should observe the loss sinking consistently. In fact you can observe that the model training hasn't fully converged yet. If you feel like you want to spend the time later to see how \n",
        "well you can get your RNN to perform, try training it for longer/until convergence. \n",
        "\n",
        "Once we have trained the model it will be interesting to use it for prediction. To generate new content we would like to feed in an initial sequence or even just a single character and see what the model generates for the rest of the sequence conditioned on our input. \n",
        "\n",
        "Let us write the logic for that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A53V0aKj2IfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model, char, device, h=None, top_k=5):\n",
        "        ''' Given a character & hidden state, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[model.char2int[char]]])\n",
        "        x = one_hot_encode(x, model.n_chars)\n",
        "        inputs = torch.from_numpy(x).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # get the output of the model\n",
        "            out, h = model(inputs, h)\n",
        "\n",
        "            # get the character probabilities\n",
        "            # move to cpu for further processing with numpy etc. \n",
        "            p = F.softmax(out, dim=1).data.cpu()\n",
        "\n",
        "            # get the top characters with highest likelihood\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "            # select the likely next character with some element of randomness\n",
        "            # for more variability\n",
        "            p = p.numpy().squeeze()\n",
        "            char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return model.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHAMT5xTlYY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(model, size, device, prime='A', top_k=None):\n",
        "    # method to generate new text based on a \"prime\"/initial sequence. \n",
        "    # Basically, the outer loop convenience function that calls the above\n",
        "    # defined predict method. \n",
        "    model.eval() # eval mode\n",
        "    \n",
        "    # Calculate model for the initial prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    with torch.no_grad():\n",
        "        # initialize hidden with 0 in the beginning. Set our batch size to 1 \n",
        "        # as we wish to generate one sequence only. \n",
        "        h = model.init_hidden(batch_size=1)\n",
        "        for ch in prime:\n",
        "            char, h = predict(model, ch, device, h=h, top_k=top_k)\n",
        "\n",
        "        # append the characters to the sequence\n",
        "        chars.append(char)\n",
        "\n",
        "        # Now pass in the previous/last character and get a new one\n",
        "        # Repeat this process for the desired length of the sequence to be \n",
        "        # generated\n",
        "        for ii in range(size):\n",
        "            char, h = predict(model, chars[-1], device, h=h, top_k=top_k)\n",
        "            chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wgYQs7FH2wo",
        "colab_type": "text"
      },
      "source": [
        "### Generating poems\n",
        "\n",
        "We are now set to call our sample method with our trained model, a prime sequence and a desired sequence length to be generated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HClso847laed",
        "colab_type": "code",
        "outputId": "a662c644-c5e6-440b-dec0-f1a259718aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "print(sample(model, 1000, device, prime='A', top_k=5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "And Deathing one freedes 'd higrund and blot ar foon.\n",
            "But be ou thou d do bulo,\n",
            "Hath s ary ureeerus nut, by or farse mo y heall; kum tho  ian t be nes for arenm y's spreeds be oo hune seall woe wheld touthere trom thee I sta then tore, ant ror my arl.\n",
            "\n",
            "Thaner the mine in theme,\n",
            "And then tor ourder proe?\n",
            "Whine ourusp oy,\n",
            "Which whece of onof reos nceer m suel an world, stuer wnet once,\n",
            "Buththe, to gracee, lo keep\n",
            "Tay sags piss ofe wolle suth,\n",
            "Thou allterish dow rowe dat of ald;'st ese at iruplinded allom, te thin thee, on mis thy frow thee fore thou trin s ile?\n",
            "Son es iat reasen, bedined wothin,\n",
            "And y be tar stay to my hofer;\n",
            "For seame to noured and sais f love's fown sse tho ghatn mithe, and tre tign,\n",
            "And thou hos ente-lo tig thellens rovh heur,\n",
            "Tiee to not tooms loves, in tham than ars my semutheres ithe dees mo hat, est on the, but ong hespe tay shacks having menared mallend sompendo dod ow,\n",
            "Nor mo tho guls,\n",
            "The righar wert the worwh, efatres, tove -fourt te sherr the shat roulles ol to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjkj103bICMs",
        "colab_type": "text"
      },
      "source": [
        "We can see that our RNN typically starts out correctly and sometimes is able to generate correct words but quickly goes on to generate junk as there is no long term dependencies. \n",
        "\n",
        "We will now implement a PyTorch LSTM to see how to improve upon this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPG6v0M7IVOB",
        "colab_type": "text"
      },
      "source": [
        "## Long Short Term Memory (LSTM)\n",
        "\n",
        "Let's take our recurrent neural network class that we have defined above and replace the simple RNN cell with one or even multiple stacked LSTM cells. \n",
        "\n",
        "If you want to go for the challenge you can try implementing this by hand similarly to the RNN cell we have defined. However, if you don't want to go through the tour-de-force exercise, you can go on ahead and use PyTorch's \"nn.LSTM()\" convenience method: https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM \n",
        "\n",
        "You can try using a stack of 2 LSTM hidden layers to simply replace the RNN cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dIQmxMEDs41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, chars, device, n_hidden=256, n_layers=2, drop_prob=0.5):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        # we already have this code on the top, but giving it to our model \n",
        "        # will be convenient for doing predictions later\n",
        "        # i.e. doing conversions from text to integers to one-hot & vice-versa\n",
        "        self.n_chars = len(chars)\n",
        "        self.int2char = dict(enumerate(chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        # define the LSTM\n",
        "        # we no longer need to care about wieght initialization as PyTorch\n",
        "        # will handle this for us now. \n",
        "        # When defining PyTorch's nn.LSTM() set \"batch_first=True\" to assign\n",
        "        # the batch size to the first dimension (instead of the sequence) to\n",
        "        # stay consistent with our RNN implementation and re-use our code. \n",
        "        self.lstm = nn.LSTM(self.n_chars, n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        # define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, self.n_chars)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            The inputs are x, and the hidden & cell state in a tuple. '''\n",
        "        \n",
        "        # get the outputs and the new hidden states from the LSTM. \n",
        "        # Note that the hidden variable now is a tuple of hidden and cell state\n",
        "        # in contrast to the RNN that just had the hidden state. \n",
        "        # Because we are using the PyTorch LSTM we do not need to implement\n",
        "        # the loop anymore as the sequence will be handled internally. \n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        # pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up the LSTM outputs using view so that we can process the last \n",
        "        # layer in parallel\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        # Calculate fully connected layer output that yields our class vector\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size=1):\n",
        "        ''' Initializes hidden state '''\n",
        "        # This is a convenience function so that we can initialize the hidden\n",
        "        # states to zero when we start prediction on a sequence. Every further\n",
        "        # step will then depend on the previous hidden states (c and h). \n",
        "        \n",
        "        # Create a tuple of two new tensors with sizes\n",
        "        # n_layers x batch_size x n_hidden, initialized to zero for the\n",
        "        # LSTM hidden and cell states. \n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL9kCT6TK2QT",
        "colab_type": "text"
      },
      "source": [
        "We can use the exact same code to train our newly defined LSTM model. Let's try with the same amount of hidden units and 2 LSTM cells. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW70FQwT2g_h",
        "colab_type": "code",
        "outputId": "1ced1442-36f5-45d9-fa4c-7869c4618c5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Define the model\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "model = LSTM(chars, device, n_hidden, n_layers).to(device)\n",
        "\n",
        "# Declaring the hyperparameters\n",
        "batch_size = 128\n",
        "seq_length = 100\n",
        "epochs = 300 # start with 50 or similar if you are debugging \n",
        "# train much longer if you want good results\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# train the model\n",
        "train(model, encoded, device, optimizer, criterion, epochs=epochs,\n",
        "      batch_size=batch_size, seq_length=seq_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/300: Loss: 3.2936:\n",
            "Epoch: 2/300: Loss: 3.1592:\n",
            "Epoch: 3/300: Loss: 3.1319:\n",
            "Epoch: 4/300: Loss: 3.1210:\n",
            "Epoch: 5/300: Loss: 3.1139:\n",
            "Epoch: 6/300: Loss: 3.1080:\n",
            "Epoch: 7/300: Loss: 3.1039:\n",
            "Epoch: 8/300: Loss: 3.1009:\n",
            "Epoch: 9/300: Loss: 3.0944:\n",
            "Epoch: 10/300: Loss: 3.0868:\n",
            "Epoch: 11/300: Loss: 3.0723:\n",
            "Epoch: 12/300: Loss: 3.0408:\n",
            "Epoch: 13/300: Loss: 2.9756:\n",
            "Epoch: 14/300: Loss: 2.9197:\n",
            "Epoch: 15/300: Loss: 2.8112:\n",
            "Epoch: 16/300: Loss: 2.7664:\n",
            "Epoch: 17/300: Loss: 2.7026:\n",
            "Epoch: 18/300: Loss: 2.6180:\n",
            "Epoch: 19/300: Loss: 2.5394:\n",
            "Epoch: 20/300: Loss: 2.4880:\n",
            "Epoch: 21/300: Loss: 2.4408:\n",
            "Epoch: 22/300: Loss: 2.4103:\n",
            "Epoch: 23/300: Loss: 2.3755:\n",
            "Epoch: 24/300: Loss: 2.3556:\n",
            "Epoch: 25/300: Loss: 2.3465:\n",
            "Epoch: 26/300: Loss: 2.3265:\n",
            "Epoch: 27/300: Loss: 2.3038:\n",
            "Epoch: 28/300: Loss: 2.2797:\n",
            "Epoch: 29/300: Loss: 2.2658:\n",
            "Epoch: 30/300: Loss: 2.2412:\n",
            "Epoch: 31/300: Loss: 2.2229:\n",
            "Epoch: 32/300: Loss: 2.2078:\n",
            "Epoch: 33/300: Loss: 2.1746:\n",
            "Epoch: 34/300: Loss: 2.1641:\n",
            "Epoch: 35/300: Loss: 2.1473:\n",
            "Epoch: 36/300: Loss: 2.1258:\n",
            "Epoch: 37/300: Loss: 2.1068:\n",
            "Epoch: 38/300: Loss: 2.0854:\n",
            "Epoch: 39/300: Loss: 2.0796:\n",
            "Epoch: 40/300: Loss: 2.0610:\n",
            "Epoch: 41/300: Loss: 2.0503:\n",
            "Epoch: 42/300: Loss: 2.0289:\n",
            "Epoch: 43/300: Loss: 2.0188:\n",
            "Epoch: 44/300: Loss: 1.9940:\n",
            "Epoch: 45/300: Loss: 1.9818:\n",
            "Epoch: 46/300: Loss: 1.9720:\n",
            "Epoch: 47/300: Loss: 1.9629:\n",
            "Epoch: 48/300: Loss: 1.9454:\n",
            "Epoch: 49/300: Loss: 1.9488:\n",
            "Epoch: 50/300: Loss: 1.9279:\n",
            "Epoch: 51/300: Loss: 1.9179:\n",
            "Epoch: 52/300: Loss: 1.9048:\n",
            "Epoch: 53/300: Loss: 1.8921:\n",
            "Epoch: 54/300: Loss: 1.8779:\n",
            "Epoch: 55/300: Loss: 1.8725:\n",
            "Epoch: 56/300: Loss: 1.8651:\n",
            "Epoch: 57/300: Loss: 1.8503:\n",
            "Epoch: 58/300: Loss: 1.8397:\n",
            "Epoch: 59/300: Loss: 1.8355:\n",
            "Epoch: 60/300: Loss: 1.8275:\n",
            "Epoch: 61/300: Loss: 1.8095:\n",
            "Epoch: 62/300: Loss: 1.8115:\n",
            "Epoch: 63/300: Loss: 1.7954:\n",
            "Epoch: 64/300: Loss: 1.7925:\n",
            "Epoch: 65/300: Loss: 1.7818:\n",
            "Epoch: 66/300: Loss: 1.7731:\n",
            "Epoch: 67/300: Loss: 1.7741:\n",
            "Epoch: 68/300: Loss: 1.7591:\n",
            "Epoch: 69/300: Loss: 1.7402:\n",
            "Epoch: 70/300: Loss: 1.7391:\n",
            "Epoch: 71/300: Loss: 1.7287:\n",
            "Epoch: 72/300: Loss: 1.7268:\n",
            "Epoch: 73/300: Loss: 1.7191:\n",
            "Epoch: 74/300: Loss: 1.7033:\n",
            "Epoch: 75/300: Loss: 1.7058:\n",
            "Epoch: 76/300: Loss: 1.6842:\n",
            "Epoch: 77/300: Loss: 1.6779:\n",
            "Epoch: 78/300: Loss: 1.6804:\n",
            "Epoch: 79/300: Loss: 1.6715:\n",
            "Epoch: 80/300: Loss: 1.6736:\n",
            "Epoch: 81/300: Loss: 1.6566:\n",
            "Epoch: 82/300: Loss: 1.6506:\n",
            "Epoch: 83/300: Loss: 1.6438:\n",
            "Epoch: 84/300: Loss: 1.6348:\n",
            "Epoch: 85/300: Loss: 1.6254:\n",
            "Epoch: 86/300: Loss: 1.6255:\n",
            "Epoch: 87/300: Loss: 1.6224:\n",
            "Epoch: 88/300: Loss: 1.6101:\n",
            "Epoch: 89/300: Loss: 1.6001:\n",
            "Epoch: 90/300: Loss: 1.5869:\n",
            "Epoch: 91/300: Loss: 1.5843:\n",
            "Epoch: 92/300: Loss: 1.5839:\n",
            "Epoch: 93/300: Loss: 1.5722:\n",
            "Epoch: 94/300: Loss: 1.5602:\n",
            "Epoch: 95/300: Loss: 1.5617:\n",
            "Epoch: 96/300: Loss: 1.5492:\n",
            "Epoch: 97/300: Loss: 1.5460:\n",
            "Epoch: 98/300: Loss: 1.5256:\n",
            "Epoch: 99/300: Loss: 1.5318:\n",
            "Epoch: 100/300: Loss: 1.5197:\n",
            "Epoch: 101/300: Loss: 1.5080:\n",
            "Epoch: 102/300: Loss: 1.5115:\n",
            "Epoch: 103/300: Loss: 1.5014:\n",
            "Epoch: 104/300: Loss: 1.4930:\n",
            "Epoch: 105/300: Loss: 1.4902:\n",
            "Epoch: 106/300: Loss: 1.4818:\n",
            "Epoch: 107/300: Loss: 1.4629:\n",
            "Epoch: 108/300: Loss: 1.4574:\n",
            "Epoch: 109/300: Loss: 1.4572:\n",
            "Epoch: 110/300: Loss: 1.4515:\n",
            "Epoch: 111/300: Loss: 1.4336:\n",
            "Epoch: 112/300: Loss: 1.4291:\n",
            "Epoch: 113/300: Loss: 1.4186:\n",
            "Epoch: 114/300: Loss: 1.4009:\n",
            "Epoch: 115/300: Loss: 1.3899:\n",
            "Epoch: 116/300: Loss: 1.3801:\n",
            "Epoch: 117/300: Loss: 1.3658:\n",
            "Epoch: 118/300: Loss: 1.3653:\n",
            "Epoch: 119/300: Loss: 1.3459:\n",
            "Epoch: 120/300: Loss: 1.3347:\n",
            "Epoch: 121/300: Loss: 1.3255:\n",
            "Epoch: 122/300: Loss: 1.3294:\n",
            "Epoch: 123/300: Loss: 1.3100:\n",
            "Epoch: 124/300: Loss: 1.2952:\n",
            "Epoch: 125/300: Loss: 1.3071:\n",
            "Epoch: 126/300: Loss: 1.2818:\n",
            "Epoch: 127/300: Loss: 1.2780:\n",
            "Epoch: 128/300: Loss: 1.2714:\n",
            "Epoch: 129/300: Loss: 1.2607:\n",
            "Epoch: 130/300: Loss: 1.2580:\n",
            "Epoch: 131/300: Loss: 1.2622:\n",
            "Epoch: 132/300: Loss: 1.2399:\n",
            "Epoch: 133/300: Loss: 1.2478:\n",
            "Epoch: 134/300: Loss: 1.2287:\n",
            "Epoch: 135/300: Loss: 1.1970:\n",
            "Epoch: 136/300: Loss: 1.1801:\n",
            "Epoch: 137/300: Loss: 1.1773:\n",
            "Epoch: 138/300: Loss: 1.1485:\n",
            "Epoch: 139/300: Loss: 1.1476:\n",
            "Epoch: 140/300: Loss: 1.1410:\n",
            "Epoch: 141/300: Loss: 1.1242:\n",
            "Epoch: 142/300: Loss: 1.1164:\n",
            "Epoch: 143/300: Loss: 1.0865:\n",
            "Epoch: 144/300: Loss: 1.0809:\n",
            "Epoch: 145/300: Loss: 1.0665:\n",
            "Epoch: 146/300: Loss: 1.0730:\n",
            "Epoch: 147/300: Loss: 1.0681:\n",
            "Epoch: 148/300: Loss: 1.0480:\n",
            "Epoch: 149/300: Loss: 1.0294:\n",
            "Epoch: 150/300: Loss: 1.0302:\n",
            "Epoch: 151/300: Loss: 1.0171:\n",
            "Epoch: 152/300: Loss: 1.0090:\n",
            "Epoch: 153/300: Loss: 1.0128:\n",
            "Epoch: 154/300: Loss: 0.9784:\n",
            "Epoch: 155/300: Loss: 0.9860:\n",
            "Epoch: 156/300: Loss: 0.9637:\n",
            "Epoch: 157/300: Loss: 0.9489:\n",
            "Epoch: 158/300: Loss: 0.9289:\n",
            "Epoch: 159/300: Loss: 0.9223:\n",
            "Epoch: 160/300: Loss: 0.9191:\n",
            "Epoch: 161/300: Loss: 0.8887:\n",
            "Epoch: 162/300: Loss: 0.9018:\n",
            "Epoch: 163/300: Loss: 0.8932:\n",
            "Epoch: 164/300: Loss: 0.8833:\n",
            "Epoch: 165/300: Loss: 0.8723:\n",
            "Epoch: 166/300: Loss: 0.8611:\n",
            "Epoch: 167/300: Loss: 0.8437:\n",
            "Epoch: 168/300: Loss: 0.8167:\n",
            "Epoch: 169/300: Loss: 0.8029:\n",
            "Epoch: 170/300: Loss: 0.7964:\n",
            "Epoch: 171/300: Loss: 0.7897:\n",
            "Epoch: 172/300: Loss: 0.7835:\n",
            "Epoch: 173/300: Loss: 0.7597:\n",
            "Epoch: 174/300: Loss: 0.7440:\n",
            "Epoch: 175/300: Loss: 0.7279:\n",
            "Epoch: 176/300: Loss: 0.7254:\n",
            "Epoch: 177/300: Loss: 0.7137:\n",
            "Epoch: 178/300: Loss: 0.7039:\n",
            "Epoch: 179/300: Loss: 0.6916:\n",
            "Epoch: 180/300: Loss: 0.6805:\n",
            "Epoch: 181/300: Loss: 0.6629:\n",
            "Epoch: 182/300: Loss: 0.6702:\n",
            "Epoch: 183/300: Loss: 0.6605:\n",
            "Epoch: 184/300: Loss: 0.6562:\n",
            "Epoch: 185/300: Loss: 0.6343:\n",
            "Epoch: 186/300: Loss: 0.6363:\n",
            "Epoch: 187/300: Loss: 0.6266:\n",
            "Epoch: 188/300: Loss: 0.6082:\n",
            "Epoch: 189/300: Loss: 0.6242:\n",
            "Epoch: 190/300: Loss: 0.6045:\n",
            "Epoch: 191/300: Loss: 0.6082:\n",
            "Epoch: 192/300: Loss: 0.6010:\n",
            "Epoch: 193/300: Loss: 0.6133:\n",
            "Epoch: 194/300: Loss: 0.5835:\n",
            "Epoch: 195/300: Loss: 0.5619:\n",
            "Epoch: 196/300: Loss: 0.5590:\n",
            "Epoch: 197/300: Loss: 0.5568:\n",
            "Epoch: 198/300: Loss: 0.5246:\n",
            "Epoch: 199/300: Loss: 0.5446:\n",
            "Epoch: 200/300: Loss: 0.5398:\n",
            "Epoch: 201/300: Loss: 0.5283:\n",
            "Epoch: 202/300: Loss: 0.5363:\n",
            "Epoch: 203/300: Loss: 0.5400:\n",
            "Epoch: 204/300: Loss: 0.5148:\n",
            "Epoch: 205/300: Loss: 0.5158:\n",
            "Epoch: 206/300: Loss: 0.5018:\n",
            "Epoch: 207/300: Loss: 0.4751:\n",
            "Epoch: 208/300: Loss: 0.4801:\n",
            "Epoch: 209/300: Loss: 0.4660:\n",
            "Epoch: 210/300: Loss: 0.4776:\n",
            "Epoch: 211/300: Loss: 0.4489:\n",
            "Epoch: 212/300: Loss: 0.4403:\n",
            "Epoch: 213/300: Loss: 0.4492:\n",
            "Epoch: 214/300: Loss: 0.4269:\n",
            "Epoch: 215/300: Loss: 0.4241:\n",
            "Epoch: 216/300: Loss: 0.4137:\n",
            "Epoch: 217/300: Loss: 0.3996:\n",
            "Epoch: 218/300: Loss: 0.3952:\n",
            "Epoch: 219/300: Loss: 0.4108:\n",
            "Epoch: 220/300: Loss: 0.3876:\n",
            "Epoch: 221/300: Loss: 0.4002:\n",
            "Epoch: 222/300: Loss: 0.3731:\n",
            "Epoch: 223/300: Loss: 0.3786:\n",
            "Epoch: 224/300: Loss: 0.3623:\n",
            "Epoch: 225/300: Loss: 0.3555:\n",
            "Epoch: 226/300: Loss: 0.3431:\n",
            "Epoch: 227/300: Loss: 0.3309:\n",
            "Epoch: 228/300: Loss: 0.3324:\n",
            "Epoch: 229/300: Loss: 0.3303:\n",
            "Epoch: 230/300: Loss: 0.3353:\n",
            "Epoch: 231/300: Loss: 0.3208:\n",
            "Epoch: 232/300: Loss: 0.3199:\n",
            "Epoch: 233/300: Loss: 0.3088:\n",
            "Epoch: 234/300: Loss: 0.3027:\n",
            "Epoch: 235/300: Loss: 0.3126:\n",
            "Epoch: 236/300: Loss: 0.3092:\n",
            "Epoch: 237/300: Loss: 0.3052:\n",
            "Epoch: 238/300: Loss: 0.2861:\n",
            "Epoch: 239/300: Loss: 0.2869:\n",
            "Epoch: 240/300: Loss: 0.2886:\n",
            "Epoch: 241/300: Loss: 0.3025:\n",
            "Epoch: 242/300: Loss: 0.2871:\n",
            "Epoch: 243/300: Loss: 0.2888:\n",
            "Epoch: 244/300: Loss: 0.2847:\n",
            "Epoch: 245/300: Loss: 0.2716:\n",
            "Epoch: 246/300: Loss: 0.2640:\n",
            "Epoch: 247/300: Loss: 0.2664:\n",
            "Epoch: 248/300: Loss: 0.2636:\n",
            "Epoch: 249/300: Loss: 0.2639:\n",
            "Epoch: 250/300: Loss: 0.2590:\n",
            "Epoch: 251/300: Loss: 0.2493:\n",
            "Epoch: 252/300: Loss: 0.2453:\n",
            "Epoch: 253/300: Loss: 0.2443:\n",
            "Epoch: 254/300: Loss: 0.2565:\n",
            "Epoch: 255/300: Loss: 0.2336:\n",
            "Epoch: 256/300: Loss: 0.2309:\n",
            "Epoch: 257/300: Loss: 0.2268:\n",
            "Epoch: 258/300: Loss: 0.2165:\n",
            "Epoch: 259/300: Loss: 0.2175:\n",
            "Epoch: 260/300: Loss: 0.2184:\n",
            "Epoch: 261/300: Loss: 0.2144:\n",
            "Epoch: 262/300: Loss: 0.2164:\n",
            "Epoch: 263/300: Loss: 0.2208:\n",
            "Epoch: 264/300: Loss: 0.2031:\n",
            "Epoch: 265/300: Loss: 0.2062:\n",
            "Epoch: 266/300: Loss: 0.1997:\n",
            "Epoch: 267/300: Loss: 0.2129:\n",
            "Epoch: 268/300: Loss: 0.2004:\n",
            "Epoch: 269/300: Loss: 0.1976:\n",
            "Epoch: 270/300: Loss: 0.1929:\n",
            "Epoch: 271/300: Loss: 0.1930:\n",
            "Epoch: 272/300: Loss: 0.1928:\n",
            "Epoch: 273/300: Loss: 0.1918:\n",
            "Epoch: 274/300: Loss: 0.1884:\n",
            "Epoch: 275/300: Loss: 0.1784:\n",
            "Epoch: 276/300: Loss: 0.1895:\n",
            "Epoch: 277/300: Loss: 0.1823:\n",
            "Epoch: 278/300: Loss: 0.1770:\n",
            "Epoch: 279/300: Loss: 0.1822:\n",
            "Epoch: 280/300: Loss: 0.1855:\n",
            "Epoch: 281/300: Loss: 0.1747:\n",
            "Epoch: 282/300: Loss: 0.1789:\n",
            "Epoch: 283/300: Loss: 0.1710:\n",
            "Epoch: 284/300: Loss: 0.1668:\n",
            "Epoch: 285/300: Loss: 0.1652:\n",
            "Epoch: 286/300: Loss: 0.1658:\n",
            "Epoch: 287/300: Loss: 0.1687:\n",
            "Epoch: 288/300: Loss: 0.1574:\n",
            "Epoch: 289/300: Loss: 0.1635:\n",
            "Epoch: 290/300: Loss: 0.1603:\n",
            "Epoch: 291/300: Loss: 0.1593:\n",
            "Epoch: 292/300: Loss: 0.1618:\n",
            "Epoch: 293/300: Loss: 0.1540:\n",
            "Epoch: 294/300: Loss: 0.1492:\n",
            "Epoch: 295/300: Loss: 0.1590:\n",
            "Epoch: 296/300: Loss: 0.1480:\n",
            "Epoch: 297/300: Loss: 0.1478:\n",
            "Epoch: 298/300: Loss: 0.1506:\n",
            "Epoch: 299/300: Loss: 0.1392:\n",
            "Epoch: 300/300: Loss: 0.1397:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTznlDdJK81w",
        "colab_type": "text"
      },
      "source": [
        "We can observe that our model is able to achieve a much lower loss than before with our simple RNN implementation. This should now also be reflected when we generate/sample new sonnets. \n",
        "\n",
        "Again, you can observe that the loss still continues to improve, even after 300 epochs. For the best results, try training the model longer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4pmGQ5J2lmJ",
        "colab_type": "code",
        "outputId": "27a825ff-0755-45f1-8a9c-55e514fbcbda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# Generating new text\n",
        "print(sample(model, 1000, device, prime='A', top_k=5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Are you mank thom more.\n",
            "Then the eoun his bounty cannot be so,\n",
            "That is the wandon berthe anowers her,\n",
            "Doth beauty tell, that arlest show at,\n",
            "And to the pares of may shat in the wrong\n",
            "Of mind, of etter's roge' to condeind;\n",
            "Sinc  and troust love have in their chist'd his.\n",
            "\n",
            "Th se can be thee is a mans romm chere.\n",
            "The her enly pay sith you day rais their.\n",
            "\n",
            "Lo, in the orte there in my love's pride,\n",
            "That selm pulse praysed it aster my sleak.\n",
            "\n",
            "So is the pirety dats on there thou most,\n",
            "And longhing still be their arase crest,\n",
            "Which I when be the set if yee to make,\n",
            "That thou athing they bequict to showe\n",
            "The obe dost dost you hos upor thy days,\n",
            "Whil thou dost hine in merter andere thated,\n",
            "That this my loves not frangers me to to grow.\n",
            "\n",
            "Leows my almaste, My such merurd but bear,\n",
            "And reath the cenfle dederion of this sealt;\n",
            "Though their thing thy wanter bear hom of srumin.\n",
            "\n",
            "My graas sheme dath tith that this thee ast.\n",
            "\n",
            "Oul do shall nowers by the dastes in the store;\n",
            "So thou wilt heary were in me sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXT-5Y-rLRKt",
        "colab_type": "text"
      },
      "source": [
        "Arguably there is still discrepancy to the original Shakespeare texts in our just generated examples. However, if we compare this output to our RNN's output, we can see that the LSTM is able to achieve much more consistency in generating proper words as well as sometimes portions of sentences that have improved in terms of grammar. There now seems to be more overall structure. "
      ]
    }
  ]
}